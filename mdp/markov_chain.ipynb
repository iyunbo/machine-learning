{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v:\n",
      " [[0.5 0.5]]\n",
      "v_3:\n",
      " [[0.812 0.188]]\n",
      "v_10:\n",
      " [[0.83329838 0.16670162]]\n",
      "v_20:\n",
      " [[0.83333333 0.16666667]]\n",
      "v_50:\n",
      " [[0.83333333 0.16666667]]\n",
      "v_100:\n",
      " [[0.83333333 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "\n",
    "v = np.array([\n",
    "    [0.5, 0.5]\n",
    "])\n",
    "\n",
    "T = np.array([\n",
    "    [0.9, 0.1],\n",
    "    [0.5, 0.5]\n",
    "])\n",
    "\n",
    "T_3 = lin.matrix_power(T, 3)\n",
    "T_10 = lin.matrix_power(T, 10)\n",
    "T_20 = lin.matrix_power(T, 20)\n",
    "T_50 = lin.matrix_power(T, 50)\n",
    "T_100 = lin.matrix_power(T, 100)\n",
    "\n",
    "print(\"v:\\n\", v)\n",
    "print(\"v_3:\\n\", str(np.dot(v, T_3)))\n",
    "print(\"v_10:\\n\", str(np.dot(v, T_10)))\n",
    "print(\"v_20:\\n\", str(np.dot(v, T_20)))\n",
    "print(\"v_50:\\n\", str(np.dot(v, T_50)))\n",
    "print(\"v_100:\\n\", str(np.dot(v, T_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of state (1,1): 0.7056\n"
     ]
    }
   ],
   "source": [
    "def utility(v, T, u, reward, gamma):\n",
    "    \"\"\"Return the state utility.\n",
    "\n",
    "    @param v the state vector\n",
    "    @param T transition matrix\n",
    "    @param u utility vector\n",
    "    @param reward for that state\n",
    "    @param gamma discount factor\n",
    "    @return the utility of the state\n",
    "    \"\"\"\n",
    "    action_utility = np.zeros(4)\n",
    "    for action in range(0,4):\n",
    "        action_utility[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    future_utility = max(action_utility)\n",
    "    return reward + gamma * future_utility\n",
    "\n",
    "def run():\n",
    "    #Starting state vector\n",
    "    #The agent starts from (1, 1)\n",
    "    v = np.array([[0.0, 0.0, 0.0, 0.0, \n",
    "                   0.0, 0.0, 0.0, 0.0, \n",
    "                   1.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "    #Transition matrix loaded from file\n",
    "    #(It is too big to write here)\n",
    "    T = np.load(\"T.npy\")\n",
    "\n",
    "    #Utility vector\n",
    "    u = np.array([[0.812, 0.868, 0.918,   1.0,\n",
    "                   0.762,   0.0, 0.660,  -1.0,\n",
    "                   0.705, 0.655, 0.611, 0.388]])\n",
    "\n",
    "    #Defining the reward for state (1,1)\n",
    "    reward = -0.04\n",
    "    #Assuming that the discount factor is equal to 1.0\n",
    "    gamma = 1.0\n",
    "\n",
    "    #Use the Bellman equation to find the utility of state (1,1)\n",
    "    utility_11 = utility(v, T, u, reward, gamma)\n",
    "    print(\"Utility of state (1,1): \" + str(utility_11))\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== FINAL RESULT ==================\n",
      "Iterations: 36\n",
      "Delta: 4.681293019892507e-09\n",
      "Gamma: 0.9999\n",
      "Epsilon: 0.0001\n",
      "===================================================\n",
      "[0.81119816 0.86756709 0.91768055 1.        ]\n",
      "[ 0.76109801  0.          0.66008269 -1.        ]\n",
      "[0.70474406 0.65465721 0.61074329 0.38727988]\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    states_count = 12\n",
    "\n",
    "    T = np.load(\"T.npy\")\n",
    "\n",
    "    u = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0])\n",
    "    \n",
    "    r = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                  -0.04,   0.0, -0.04,  -1.0,\n",
    "                  -0.04, -0.04, -0.04, -0.04])\n",
    "    gamma = 0.9999\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        u0 = u.copy()\n",
    "        iteration += 1\n",
    "        for s in range(states_count):\n",
    "            reward = r[s]\n",
    "            v = np.zeros((1, states_count))\n",
    "            v[0,s] = 1.0\n",
    "            u[s] = calculate_utility(v, T, u0, reward, gamma)\n",
    "            delta = max(delta, np.abs(u[s] - u0[s]))\n",
    "            \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "                print(\"=================== FINAL RESULT ==================\")\n",
    "                print(\"Iterations: \" + str(iteration))\n",
    "                print(\"Delta: \" + str(delta))\n",
    "                print(\"Gamma: \" + str(gamma))\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "                print(\"===================================================\")\n",
    "                print(u[0:4])\n",
    "                print(u[4:8])\n",
    "                print(u[8:12])\n",
    "                print(\"===================================================\")\n",
    "                break    \n",
    "                \n",
    "value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(p, u, r, T, gamma):\n",
    "    \"\"\"\n",
    "    Return the policy utility.\n",
    "    @param p policy vector\n",
    "    @param u utility vector\n",
    "    @param r reward vector\n",
    "    @param T transition matrix\n",
    "    @param gamma discount factor\n",
    "    @return the utility vector u\n",
    "    \"\"\"\n",
    "    for s in range(12):\n",
    "        if np.isnan(p[s]): \n",
    "            continue\n",
    "        v = np.zeros((1,12))\n",
    "        v[0,s] = 1.0\n",
    "        action = int(p[s])\n",
    "        u[s] = r[s] + gamma * np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    \n",
    "    return u\n",
    "\n",
    "def expected_action(u, T, v):\n",
    "    \"\"\"Return the expected action.\n",
    "\n",
    "    It returns an action based on the\n",
    "    expected utility of doing a in state s, \n",
    "    according to T and u. This action is\n",
    "    the one that maximize the expected\n",
    "    utility.\n",
    "    @param u utility vector\n",
    "    @param T transition matrix\n",
    "    @param v starting vector\n",
    "    @return expected action (int)\n",
    "    \"\"\"\n",
    "    actions_utility = np.zeros(4)\n",
    "    for action in range(4):\n",
    "        actions_utility[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    \n",
    "    return np.argmax(actions_utility)\n",
    "\n",
    "def print_policy(p, shape):\n",
    "    \"\"\"Printing utility.\n",
    "\n",
    "    Print the policy actions using symbols:\n",
    "    ^, v, <, > up, down, left, right\n",
    "    * terminal states\n",
    "    # obstacles\n",
    "    \"\"\"\n",
    "    state = 0\n",
    "    policy_string = \"\"\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            if(p[state] == -1): policy_string += \" *  \"            \n",
    "            elif(p[state] == 0): policy_string += \" ^  \"\n",
    "            elif(p[state] == 1): policy_string += \" <  \"\n",
    "            elif(p[state] == 2): policy_string += \" v  \"           \n",
    "            elif(p[state] == 3): policy_string += \" >  \"\n",
    "            elif(np.isnan(p[state])): policy_string += \" #  \"\n",
    "            state += 1\n",
    "        policy_string += '\\n'\n",
    "    print(policy_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== FINAL RESULT ==================\n",
      "Iterations: 25\n",
      "Delta: 3.878557808256744e-09\n",
      "Gamma: 0.9999\n",
      "Epsilon: 0.0001\n",
      "===================================================\n",
      "[0.81119816 0.86756709 0.91768055 1.        ]\n",
      "[ 0.76109801  0.          0.66008269 -1.        ]\n",
      "[0.70474405 0.65465721 0.61074329 0.38727989]\n",
      "===================================================\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration():\n",
    "    states_count = 12\n",
    "\n",
    "    T = np.load(\"T.npy\")\n",
    "\n",
    "    u = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                  0.0, 0.0, 0.0,  0.0,\n",
    "                  0.0, 0.0, 0.0,  0.0])\n",
    "    \n",
    "    r = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                  -0.04,   0.0, -0.04,  -1.0,\n",
    "                  -0.04, -0.04, -0.04, -0.04])\n",
    "    \n",
    "    p = np.random.randint(0, 4, size=(12)).astype(np.float32)\n",
    "    p[5] = np.NaN\n",
    "    p[3] = p[7] = -1\n",
    "    \n",
    "    gamma = 0.9999\n",
    "    epsilon = 0.0001\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        #1- Policy evaluation\n",
    "        u0= u.copy()\n",
    "        u = policy_evaluation(p, u, r, T, gamma)\n",
    "        delta = np.absolute(u - u0).max()\n",
    "        \n",
    "        if delta < epsilon * (1 - gamma) / gamma: break\n",
    "        \n",
    "        for s in range(12):\n",
    "            if not np.isnan(p[s]) and p[s]!=-1:\n",
    "                v = np.zeros((1,12))\n",
    "                v[0,s] = 1.0\n",
    "                #2- Policy improvement    \n",
    "                p[s] = return_expected_action(u, T, v)\n",
    "                \n",
    "    print(\"=================== FINAL RESULT ==================\")\n",
    "    print(\"Iterations: \" + str(iteration))\n",
    "    print(\"Delta: \" + str(delta))\n",
    "    print(\"Gamma: \" + str(gamma))\n",
    "    print(\"Epsilon: \" + str(epsilon))\n",
    "    print(\"===================================================\")\n",
    "    print(u[0:4])\n",
    "    print(u[4:8])\n",
    "    print(u[8:12])\n",
    "    print(\"===================================================\")\n",
    "    print_policy(p, shape=(3,4))\n",
    "    print(\"===================================================\")\n",
    "    \n",
    "policy_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
